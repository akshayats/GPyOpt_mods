import numpy as np
from ..util.general import multigrid, samples_multidimensional_uniform, reshape
from scipy.stats import norm
import scipy
import GPyOpt


def random_batch_optimization(acquisition, bounds, acqu_optimize_restarts, acqu_optimize_method, model, n_inbatch):
    '''
    Computes de batch optimization taking random samples (only for comparative purposes)
    '''
    X_batch = optimize_acquisition(acquisition, bounds, acqu_optimize_restarts, acqu_optimize_method, model)
    k=1 
    while k<n_inbatch:
        new_sample = samples_multidimensional_uniform(bounds,1)
        X_batch = np.vstack((X_batch,new_sample))  
        k +=1
    return X_batch


def adaptive_batch_optimization(acquisition, bounds, acqu_optimize_restarts, acqu_optimize_method, model, n_inbatch, alpha_L, alpha_Min):
    '''
    Computes batch optimzation using by acquisition penalization using Lipschizt inference
    :param acquisition: acquisition function in which the batch selection is based
    :param bounds: the box constrains of the optimization
    :restarts: the number of restarts in the optimization of the surrogate
    :method: the method to optimize the aquisition function
    :model: the GP model based on the current samples
    :n_inbatch: the number of samples to collect
    :alpha_L: z quantile for the estimation of the lipchiz constant L
    :alpha_Min: z quantile for the estimation of the minimum Min
    '''
    X_batch = optimize_acquisition(acquisition, bounds, acqu_optimize_restarts, acqu_optimize_method, model, X_batch=None, L=None, Min=None)
    k=1
    if n_inbatch>1:
        L = estimate_L(model,bounds,alpha_L)		# Estimation of the Lipschizt constat
        Min = estimate_Min(model,bounds,alpha_Min)      # Estimation of the minimum value

    while k<n_inbatch:
        new_sample = optimize_acquisition(acquisition, bounds, acqu_optimize_restarts, acqu_optimize_method, model, X_batch, L, Min)
        X_batch = np.vstack((X_batch,new_sample))  
        k +=1
    return X_batch


def hybrid_batch_optimization(acqu_name, acquisition_par, acquisition, bounds, acqu_optimize_restarts, acqu_optimize_method, model, n_inbatch):   
    '''
    Computes batch optimzation using by acquisition penalization using Lipschizt inference
    :param acquisition: acquisition function in which the batch selection is based
    :param bounds: the box constrains of the optimization
    :restarts: the number of restarts in the optimization of the surrogate
    :method: the method to optimize the aquisition function
    :model: the GP model based on the current samples
    :n_inbatch: the number of samples to collect
    :alpha_L: z quantile for the estimation of the lipchiz constant L
    :alpha_Min: z quantile for the estimation of the minimum Min
    '''
    model_copy = model.copy()
    X = model_copy.X 
    Y = model_copy.Y
    input_dim = X.shape[1] 
    kernel = model_copy.kern    
    X_new = optimize_acquisition(acquisition, bounds, acqu_optimize_restarts, acqu_optimize_method, model, X_batch=None, L=None, Min=None)
    X_batch = reshape(X_new,input_dim)
    k=1
    while k<n_inbatch:
        X = np.vstack((X,reshape(X_new,input_dim)))       # update the sample within the batch
        Y = np.vstack((Y,model.predict(reshape(X_new, input_dim))[0]))
       
        try: # this exception is included in case two equal points are selected in a batch, in this case the method stops
            batchBO = GPyOpt.methods.BayesianOptimization(f=0, 
                                        bounds= bounds, 
                                        X=X, 
                                        Y=Y, 
                                        kernel = kernel,
                                        acquisition = acqu_name, 
                                        acquisition_par = acquisition_par)
        except np.linalg.linalg.LinAlgError:
            print 'Optimization stopped. Two equal points selected.'
            break        

        batchBO.start_optimization(max_iter = 0, 
                                    n_inbatch=1, 
                                    acqu_optimize_method = acqu_optimize_method,  
                                    acqu_optimize_restarts = acqu_optimize_restarts, 
                                    stop_criteria = 1e-6)
        
        X_new = batchBO.suggested_sample
        X_batch = np.vstack((X_batch,X_new))
        model_batch = batchBO.model
        k+=1    
    return X_batch

def estimate_L(model,bounds,alpha=0.025):
    def df(x,model,alpha):
        x = reshape(x,model.X.shape[1])
        dmdx, dsdx = model.predictive_gradients(x)
        res = np.sqrt((dmdx*dmdx).sum(1)) # simply take the norm of the expectation of the gradient
        return -res
   
    samples = samples_multidimensional_uniform(bounds,5)
    pred_samples = df(samples,model,alpha)
    x0 = samples[np.argmin(pred_samples)]
    minusL = scipy.optimize.minimize(df,x0, method='SLSQP',bounds=bounds, args = (model,alpha)).fun[0][0]
    L = -minusL
    if L<0.1: L=100  ## to avoid problems in cases in which the mode is flat
    return L

# Estimates 'the minimum' of a model
#def estimate_Min(model,bounds,alpha=0.025):
#    def f(x,model,alpha):
#        if len(x.flatten())==2:
#            x = x.reshape(1,2)
#        m,v = model.predict(x)
#        res = m #+ norm.ppf(1-alpha)*np.sqrt(abs(v))
#        return res
#    samples = samples_multidimensional_uniform(bounds,25)
#    pred_samples = f(samples,model,alpha)
#    x0 = samples[np.argmin(pred_samples)]
#    return scipy.optimize.minimize(f,x0, method='SLSQP',bounds=bounds, args = (model,alpha)).fun[0][0]

def estimate_Min(model,bounds,alpha=0.025):
    return model.Y.min()


# Creates the function to define the esclusion zones
def hammer_function(x,x0,L,Min,model):
    x0 = x0.reshape(1,len(x0))
    m = model.predict(x0)[0]
    s = np.sqrt(model.predict(x0)[1])
    r_x0 = (m-Min)/L
    s_x0 = s/L
    return (norm.cdf((np.sqrt(((x-x0)**2).sum(1))- r_x0)/s_x0)).T


# Creates a penalized acquisition function using 'hammer' functions around the points collected in the batch
def penalized_acquisition(x, acquisition, bounds, model, X_batch=None, L=None, Min=None):
    sur_min = min(-acquisition(model.X))  # assumed minimum of the minus acquisition
    fval = -acquisition(x)-np.sign(sur_min)*(abs(sur_min)) 
    if X_batch!=None:
        X_batch = reshape(X_batch,model.X.shape[1]) ## !!!!!! model.X.shape[1] insted of 2
        for i in range(X_batch.shape[0]):            
            fval = np.multiply(fval,hammer_function(x, X_batch[i,], L, Min, model))
    return -fval

### Optimization of the aquisition function
def optimize_acquisition(acquisition, bounds, acqu_optimize_restarts, acqu_optimize_method, model, X_batch=None, L=None, Min=None):
    if acqu_optimize_method=='brute':
        res = full_acquisition_optimization(acquisition,bounds,acqu_optimize_restarts, model, 'brute', X_batch, L, Min)
    elif acqu_optimize_method=='random':
        res =  full_acquisition_optimization(acquisition,bounds,acqu_optimize_restarts, model, 'random', X_batch, L, Min)
    elif acqu_optimize_method=='fast_brute':
        res =  fast_acquisition_optimization(acquisition,bounds,acqu_optimize_restarts, model, 'brute', X_batch, L, Min)
    elif acqu_optimize_method=='fast_random':
        res =  fast_acquisition_optimization(acquisition,bounds,acqu_optimize_restarts, model, 'random', X_batch, L, Min)
    return res

### Optimizes the acquisition function using a local optimizer in the best point
def fast_acquisition_optimization(acquisition, bounds,acqu_optimize_restarts, model, method_type, X_batch=None, L=None, Min=None):
    if method_type=='random':
                samples = samples_multidimensional_uniform(bounds,acqu_optimize_restarts)
    else:
        samples = multigrid(bounds, acqu_optimize_restarts)
    pred_samples = acquisition(samples)
    x0 =  samples[np.argmin(pred_samples)]
    res = scipy.optimize.minimize(penalized_acquisition, x0=np.array(x0),method='SLSQP',bounds=bounds, args=(acquisition, bounds, model, X_batch, L, Min))
    return res.x

### Optimizes the acquisition function by taking the best of a number of local optimizers
def full_acquisition_optimization(acquisition, bounds, acqu_optimize_restarts, model, method_type, X_batch=None, L=None, Min=None):
    if method_type=='random':
        samples = samples_multidimensional_uniform(bounds,acqu_optimize_restarts)
    else:
        samples = multigrid(bounds, acqu_optimize_restarts)
    mins = np.zeros((acqu_optimize_restarts,len(bounds)))
    fmins = np.zeros(acqu_optimize_restarts)
    for k in range(acqu_optimize_restarts):
        res = scipy.optimize.minimize(penalized_acquisition, x0 = samples[k,:] ,method='SLSQP',bounds=bounds, args=(acquisition, bounds, model, X_batch, L, Min))
        mins[k] = res.x
        fmins[k] = res.fun
    return mins[np.argmin(fmins)]



